<!DOCTYPE html>
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta charset="utf-8" />
    <link rel="stylesheet" href="global.css" />
    <link rel="stylesheet" href="style.css" />
    <script src="https://kit.fontawesome.com/4a44b6177d.js" crossorigin="anonymous"></script>
  </head>
  <body>
    <div class="desktop">
      <div class="banner">
        
        
        <a href="index.html" style="text-decoration: none;">
        <div class="text-wrapper" id="Paragon">PARAGON</div>
        </a>
    
    </div>
    <div class="Main">
      <div class="title">
        <h2>DATA PREPERATION</h2>
          <div class="title-line"></div>
      </div>
      <div class="text-bubble">
      <p>
All of our data comes from the <a href="https://huggingface.co/datasets/amaai-lab/DisfluencySpeech/viewer/default/train?row=87">Hugging Face Disfluency Speech </a> dataset (Wang, K., & Herremans, D., 2024). This dataset includes columns of raw audio .wav files as well as four transcriptions with varying degrees of filtration. The audio files range from ~5-10 seconds. The column ‘transcript_annotated’ contains non-speech elements, filler words, repeated words, and restarts. The column ‘transcript_a’ removes non-speech elements, ‘transcript_b’ removes filler words, and ‘transcript_c’ removes repeated words and restarts. An example of the first row of the train data is shown below in Figure 1. 
      </p>
      <br>
      <figure>
        <img src="img/fig1.jpeg", alt="Hugging Face Disfluency Speech dataset">
          <figcaption>Figure 1: The first row of Hugging Face's Disfluency Speech dataset</figcaption>
      </figure>
      <p>
This project is at the moment limited by the amount of data used, as this dataset only contains 5,000 entries with a 90/5/5 train/test/val split. First steps at improving the application would be to expand the amount of data both the speech to text and the filtration models were trained on. While both of these models utilized pre-training on larger models, they are likely overfitted to the specific dataset they were fine-tuned on.      </p>
      <br>
      <h4>Whisper model</h4>
      <p>
  For the whisper model the data had to be transformed into a standard format of  pairs constant of transcript and audio. In Figure 2 the first five rows of the downloaded dataset are shown. As we can see the audio is stored in bytes, while the whisper model expects mp3, mp4, mpweg, mpga, m4a, wav, or  webm format. For that purpose the data had to be transformed, we chose wav as we have previous experience working with it. Then for each row in the parquet files, the audio and transcript was taken, the audio was transformed and saved to a folder and its path was stored in a list. After all rows were processed and in the same list, the list was reshuffled and then split into a new ratio of train, test and validation. The chosen ratio was 70% training, 15% testing and 15% validation, which allows better testing and validation of the model. The splitted data was saved as pandas dataframes. 
      </p>
      <br>
      <p>
        Whisper uses an encoder-decoder architecture where the encoder decrypts the audio while the decoder uses the decrypted audio for generating an output. When encoding audiofiles, whisper utilizes log mel spectogram, which is a frequency representation of audio as an image. Additionally, the whisper decoder doesn't generate text output, it generates numerical values where each value corresponds to a word. For this project all whisper related parts such as the processor, tokenizer etc. was taken from the transformer library by HuggingFace. For training the whisper model it expects the log spectrogram and not the raw audio, tokenized words instead of strings and Hugginface expects datasets objects instead of pandas dataframes. Therefore, the data was first converted to dataset objects with the from_pandas() function, then the  processed audio data had to be converted to a log mel spectrum using whispers processor, while the processed transcript had to be tokenized.
      </p>

         <br>
      <figure id="fig2">
        <img src="img/fig2.jpeg", alt="Loaded data">
          <figcaption>Figure 2: The first five rows of downloaded dataset</figcaption>
      </figure>

      <h4>Filtration model</h4>
      <p>
For the filtration model, the data was preprocessed by stripping each entry and putting them in lower-case. Then, certain punctuation marks (,.?!:;) were separated from surrounding words in order to treat them as their own tokens for the model. Other special characters were removed by the tokenizers. For the LSTM model, the tokenizer used was from the tf.keras.preprocessing library. For the transformer, the tokenizer used was the Autotokenizer from the transformers library. The start and end of sequence tokens were 101 and 102, respectively, for the transformer model. The latter tokenizer subdivides words into subwords, and so each subword was ensured to have the same label as each other subword for a given token.      </p>
      <br>
      <p>
Output data was converted to a binary keep/remove classifier for each word. This used difflib to compare the raw and the clean text output, and assigned each token a 1 if it did not appear in the clean text and a 2 if it did, as well as appending enough 0s to the end of the sequence for padding. The max length for padding was determined by examining the size of each sentence after pre-processing and concluding that most of the sentences fell below 40 words, as shown in Figure 3. The proportion of each label was examined to ensure that there was not too much padding, and is shown in Figure 4. 
      </p>

      <br>

        <figure id="fig3">
        <img src="img/fig3.jpeg", alt="Word per sequence">
          <figcaption>Figure 3: The number of words in each sequence of the Speech Disfluency dataset</figcaption>
      </figure>

        <figure id="fig4">
        <img src="img/fig4.jpeg", alt="Percetage of each label">
          <figcaption>Figure 4: Percentages of each label in the data after choosing a max sequence length of 40, where 0 is padding, 1 is a remove label, and 2 is a keep labelt</figcaption>
      </figure>
    </div>



  </div>
      <div class="menu-sign">
        <div class="menu">
          <div class="ellipse-2"></div>
          <div class="ellipse-3"></div>
          <div class="ellipse-4"></div>
        </div>
      </div>  
<div class="menu-dropdown" id="menuDropdown">
        <a href="index.html">Abstract</a>
          <a href="introduction.html">Introduction</a>
          <a href="data_prep.html">Data and prep</a>
          <a href="methods.html">Analysis: Models/Methods</a>
          <a href="result.html">Results</a>
          <a href="conclusion.html">Conclusion</a>
          <a href="citation.html">References</a>
</div>
    </div>

    <footer>
      <div class="footer-columns">
        <div class="footer-info">
          <p>This was a project for University of Colorado Boulder in the course Neural Networks and Deep Learning, Fall 2025.</p>
          <a href="https://github.com/emni8523/neural_networks_project">Link to Github</a>
        </div>

        <div class="footer-authors">
          <p><strong>Authors</strong></p>
          <p>Emil Nilsson</p>
          <p>Jeremy Miesch</p>
        </div>
      </div>
    </footer>

     <script>

      const menuSign = document.querySelector('.menu-sign');
      const menuDropdown = document.getElementById('menuDropdown');

      menuSign.addEventListener('click', function(e) {
        e.stopPropagation();
        menuDropdown.classList.toggle('active');
      });

      document.addEventListener('click', function(e) {
        if (!menuSign.contains(e.target)) {
          menuDropdown.classList.remove('active');
        }
      });

  </script>
  </body>
