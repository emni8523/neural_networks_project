{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Bidirectional, Input, Dropout, Layer, Reshape, Masking, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, BinaryAccuracy, F1Score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import difflib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel, AutoConfig, pipeline, WhisperFeatureExtractor, WhisperTokenizer, AutoModelForSpeechSeq2Seq, TFBertModel\n",
    "from datasets import Dataset\n",
    "\n",
    "from evaluate import load\n",
    "from openai import OpenAI\n",
    "\n",
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7905dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset from huggingface\n",
    "ds = load_dataset(\"amaai-lab/DisfluencySpeech\")\n",
    "\n",
    "train = ds[\"train\"]\n",
    "val  = ds[\"validation\"]\n",
    "test = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First approach - use a basic LSTM to predict filter words. For each word, assign 0 if removed between transcript a and transcript c, 1 if kept\n",
    "\n",
    "class LSTMModel():\n",
    "    # segmented - if true, have punctuation marks like , . ? as there own tokens\n",
    "    # lower - lower case\n",
    "    def __init__(self, segmented=True, lower=True, model_type=\"lstm\"):\n",
    "\n",
    "        # load in data \n",
    "        X_train = [x.lower().strip() for x in list(train[\"transcript_a\"])] if lower else [x.strip() for x in list(train[\"transcript_a\"])]\n",
    "        y_train = [x.lower().strip() for x in list(train[\"transcript_c\"])] if lower else [x.strip() for x in list(train[\"transcript_c\"])]\n",
    "\n",
    "        X_val = [x.lower().strip() for x in list(val[\"transcript_a\"])] if lower else [x.strip() for x in list(val[\"transcript_a\"])]\n",
    "        y_val = [x.lower().strip() for x in list(val[\"transcript_c\"])] if lower else [x.strip() for x in list(val[\"transcript_c\"])]\n",
    "\n",
    "        X_test = [x.lower().strip() for x in list(test[\"transcript_a\"])] if lower else [x.strip() for x in list(test[\"transcript_a\"])]\n",
    "        y_test = [x.lower().strip() for x in list(test[\"transcript_c\"])] if lower else [x.strip() for x in list(test[\"transcript_c\"])]\n",
    "\n",
    "        # pre_process if segmented \n",
    "        if segmented:\n",
    "            X_train = [self.pre_process_text(x) for x in X_train]\n",
    "            X_test = [self.pre_process_text(x) for x in X_test]\n",
    "            X_val = [self.pre_process_text(x) for x in X_val]\n",
    "            y_train = [self.pre_process_text(x) for x in y_train]\n",
    "            y_test = [self.pre_process_text(x) for x in y_test]\n",
    "            y_val = [self.pre_process_text(x) for x in y_val]\n",
    "\n",
    "        # tokenizer does not include , . ? in the filters if segmented is true, then it filters everything else out\n",
    "        self.tokenizer = Tokenizer(oov_token=\"oov\", filters='\"#$%&()*+-/<=>@[\\\\]^_`{|}~\\t\\n') if segmented else Tokenizer(oov_token=\"oov\")\n",
    "        self.tokenizer.fit_on_texts(X_train + y_train + X_val + y_val + X_test + y_test)\n",
    "        self.vocab_size = len(self.tokenizer.word_index)+1 #+1 for oov\n",
    "\n",
    "        # about 90% of the data is less than 40 words long\n",
    "        self.max_length = 40\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        self.segmented = segmented\n",
    "        self.lower = True\n",
    "        self.model_type = model_type\n",
    "\n",
    "\n",
    "    # use re to add a space before notable punctuation\n",
    "    def pre_process_text(self, text):\n",
    "        text = re.sub(r'([,.?!:;])',r' \\1',text)\n",
    "        return text\n",
    "\n",
    "\n",
    "    # function that takes in raw text and clean text; returns tokens of raw_text and a list of binary labels (1 for remove, 2 for keep)\n",
    "    def make_binary_labels(self, raw, clean, return_tokens=True):\n",
    "        raw_words = raw.split()\n",
    "        clean_words = clean.split()\n",
    "        \n",
    "        matcher = difflib.SequenceMatcher(None, raw_words, clean_words) #isJunk = None to ignore no items\n",
    "        labels = []\n",
    "\n",
    "        for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "            if tag == 'equal':\n",
    "                labels += [2] * (i2 - i1)   # keep\n",
    "            else:\n",
    "                labels += [1] * (i2 - i1)   # remove\n",
    "        \n",
    "        # for model 2, which uses different encodings\n",
    "        if not return_tokens:\n",
    "            return raw_words, labels\n",
    "\n",
    "        return self.tokenizer.texts_to_sequences([raw])[0], labels\n",
    "    \n",
    "\n",
    "    # prints a histogram of the number of words per sequence\n",
    "    def plot_seq_lengths(self):\n",
    "        X_train_lengths = [len(i.split()) for i in self.X_train]\n",
    "        print(f\"First few lengths: {X_train_lengths[:5]}\")\n",
    "\n",
    "        plt.hist(X_train_lengths)\n",
    "        plt.plot()\n",
    "\n",
    "\n",
    "    # take in all raw/clean data and reformat as raw_tokens/clean labels after padding\n",
    "    def encode_example(self, x, y):\n",
    "        raw_token, raw_label = self.make_binary_labels(x, y)\n",
    "\n",
    "        # pad sequences to each be the same length\n",
    "        tokens_padded = pad_sequences([raw_token], maxlen=self.max_length, padding='post')[0]\n",
    "        labels_padded = pad_sequences([raw_label], maxlen=self.max_length, padding='post')[0]\n",
    "\n",
    "        return tokens_padded, labels_padded\n",
    "    \n",
    "\n",
    "    # create a binary dataset from raw->clean dataset\n",
    "    def create_binary_dataset(self,X,Y):\n",
    "        x_toks = []\n",
    "        y_labels = []\n",
    "\n",
    "        for x,y in zip(X,Y):\n",
    "            toks, labels = self.encode_example(x,y)\n",
    "            x_toks.append(toks)\n",
    "            y_labels.append(labels)\n",
    "        \n",
    "        return np.array(x_toks), np.array(y_labels)\n",
    "\n",
    "    \n",
    "    # print the proportions of each label to check if padding makes sense \n",
    "    def print_props(self, y_labels):\n",
    "        unique, counts = np.unique(y_labels,return_counts=True)\n",
    "        for u, c in zip(unique, counts):\n",
    "            print(f\"Percentage {u} is {c / counts.sum()}\")\n",
    "\n",
    "    \n",
    "    # train the model\n",
    "    def train(self,epochs):\n",
    "        self.X_train_tokens, self.y_train_labels = self.create_binary_dataset(self.X_train, self.y_train)\n",
    "        self.X_val_tokens, self.y_val_labels = self.create_binary_dataset(self.X_val, self.y_val)\n",
    "        self.X_test_tokens, self.y_test_labels = self.create_binary_dataset(self.X_test, self.y_test)\n",
    "        \n",
    "        self.model = Sequential([\n",
    "            Embedding(input_dim=self.vocab_size, input_shape=(self.vocab_size,), output_dim=32, mask_zero=True), # masks zero so we can use sparsecategoricalcrossentropy and accuracy without the padding affecting the loss\n",
    "            Bidirectional(LSTM(units=100, dropout=0.2, return_sequences=True)),\n",
    "            Bidirectional(LSTM(units=50, dropout=0.2, return_sequences=True)),\n",
    "            Dense(3, activation='softmax') # pad, keep, remove\n",
    "        ])\n",
    "        self.model.summary()\n",
    "\n",
    "        # add sample weights to emphasize getting the removed tokens right more frequently\n",
    "        sample_weights = np.ones_like(self.y_train_labels) # same shape, all ones\n",
    "        sample_weights[self.y_train_labels==0] = 0 # this also makes the padded words not count in the accuracy calculation\n",
    "        sample_weights[self.y_train_labels==1] = 2\n",
    "        sample_weights[self.y_train_labels==2] = 1\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer= Adam(0.001), # learning rate\n",
    "            loss= SparseCategoricalCrossentropy(),\n",
    "            metrics=[SparseCategoricalAccuracy()])\n",
    "\n",
    "        callbacks = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=4, restore_best_weights=True) # implement early stopping to prevent overfitting\n",
    "        history = self.model.fit(self.X_train_tokens,\n",
    "                self.y_train_labels,\n",
    "                epochs=epochs,\n",
    "                validation_data=(self.X_val_tokens, self.y_val_labels),\n",
    "                sample_weight=sample_weights,\n",
    "                callbacks=callbacks)\n",
    "        \n",
    "        # plot loss and accuracy over epochs\n",
    "        plt.plot(history.history['sparse_categorical_accuracy'], label = 'accuracy')\n",
    "        plt.plot(history.history['val_sparse_categorical_accuracy'], label = 'val_accuracy')\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.plot()\n",
    "\n",
    "\n",
    "    # predict a raw text input, returning the predicted labels, the sentnece, and the tokenized sequence\n",
    "    def predict_sequence(self,raw_text):\n",
    "        # tokenize and pad\n",
    "        seq = self.tokenizer.texts_to_sequences([self.pre_process_text(raw_text)] if self.segmented else [raw_text])\n",
    "        seq = pad_sequences(seq, maxlen=self.max_length, padding='post')\n",
    "\n",
    "        # predict sequence\n",
    "        preds = self.model.predict(seq)[0]\n",
    "        pred_classes = np.argmax(preds, axis=-1)\n",
    "\n",
    "        # reverse word_index to get words from tokens\n",
    "        reverse_index = {v: k for k, v in self.tokenizer.word_index.items()}\n",
    "        reverse_index[0] = '<PAD>'\n",
    "\n",
    "        # only keep words labelled 2\n",
    "        pred_sentence = ''\n",
    "        for idx, label in zip(seq[0],pred_classes):\n",
    "            if idx == 0: #ignore padding\n",
    "                continue \n",
    "\n",
    "            if label == 2: # only keep 2s\n",
    "                word = reverse_index[idx]\n",
    "                pred_sentence += word + ' '\n",
    "\n",
    "        # put sentence back together\n",
    "        pred_sentence = re.sub(r' ([,.?!:;])',r'\\1',pred_sentence)\n",
    "\n",
    "        return pred_classes, pred_sentence, seq\n",
    "\n",
    "\n",
    "    # print a confusion matrix given tokens and labels\n",
    "    def print_cm(self,toks,labels):  \n",
    "        predictions = self.model.predict(toks)\n",
    "        y_pred = np.argmax(predictions, axis=-1).flatten()\n",
    "        y_gold = labels.flatten()\n",
    "\n",
    "        # remove padding values \n",
    "        mask = (y_gold != 0)\n",
    "        y_pred = y_pred[mask]\n",
    "        y_gold = y_gold[mask]\n",
    "\n",
    "        cm = confusion_matrix(y_gold, y_pred, labels=[1,2])\n",
    "\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "\n",
    "        labels = ['Remove', 'Keep']\n",
    "\n",
    "        disp.ax_.set_title(\"Filter Model\")\n",
    "        disp.ax_.set_xticklabels(labels,rotation=90)\n",
    "        disp.ax_.set_yticklabels(labels)\n",
    "\n",
    "        # use sklearn metrics \n",
    "        print(f\"F1 score: {f1_score(y_gold, y_pred)}\")\n",
    "        print(f\"Classification Report: \\n {classification_report(y_gold, y_pred)}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # evaluate on test data, predict a given sentence, and print the cm\n",
    "    def show_eval(self):\n",
    "        loss, accuracy = self.model.evaluate(self.X_test_tokens, self.y_test_labels)\n",
    "\n",
    "        print(f\"Test loss: {loss}\")\n",
    "        print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "        print(\"\\n-------------\\n\")\n",
    "\n",
    "        print(\"Testing on the sentence: i, uh, don't, i don't think that is true\")\n",
    "        print(\"Expected: i don't think that is true\")\n",
    "        print(f\"Result: {self.predict_sequence(\"i, uh, don't, i don't think that is true\")[1]}\")\n",
    "\n",
    "        self.print_cm(self.X_test_tokens,self.y_test_labels)\n",
    "\n",
    "        \n",
    "    # train an evaluate\n",
    "    def run(self, epochs):\n",
    "        self.train(epochs)\n",
    "        self.show_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ff9797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this wrapper class to avoid getting errors, bert takes in only tensorflow tensors and not keras tensors\n",
    "class BertLayer(Layer):\n",
    "    def __init__(self, model):\n",
    "        super().__init__() # init Layer class\n",
    "        self.config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n",
    "        self.bert = TFBertModel.from_pretrained( #bert model\n",
    "            model,\n",
    "            config=self.config,\n",
    "            from_pt=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states\n",
    "        layers = hidden_states[-4:] # get last 4 hidden layers\n",
    "        x = tf.concat(layers, axis=-1)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch = input_shape[0][0]\n",
    "        seq = input_shape[0][1]\n",
    "        return (batch, seq, self.config.hidden_size * 4)\n",
    "\n",
    "\n",
    "# Transformer model, inherited from bart\n",
    "class TransformerModel(LSTMModel):\n",
    "    def __init__(self, batch=8, segmented=True, lower=True, model_type=\"transformer\"):\n",
    "        super().__init__(segmented, lower, model_type) # init from parent class\n",
    "\n",
    "        # different tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.batch = batch\n",
    "    \n",
    "\n",
    "    # when assigning labels, BERT will separate words like testing into test and ing. Use word_ids here to ensure that each subword has the same class as other subwords\n",
    "    def subword_segment(self, word_ids, y_labels):\n",
    "        labels = []\n",
    "\n",
    "        for i in word_ids:\n",
    "            if i is None:\n",
    "                labels.append(0) # label special characters the same as padding\n",
    "            else:\n",
    "                labels.append(y_labels[i])\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "\n",
    "    # take in clean and raw text, and return an encoding dictionary of tensors that includes tokens ('input_ids'), labels ('labels'), and attention_mask (used during training, 1 for use a word, 0 for not (special character, padding, etc.))\n",
    "    def encode_example(self, X, y):\n",
    "        X_raw_words, y_labels = self.make_binary_labels(X,y,return_tokens=False) #return raw words instead\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(X_raw_words, \n",
    "                                        max_length=self.max_length, #est earlier\n",
    "                                        truncation=True, \n",
    "                                        padding=\"max_length\",\n",
    "                                        is_split_into_words=True,\n",
    "                                        return_token_type_ids=False, #don't need these\n",
    "                                        return_tensors='tf' #for tensorflow\n",
    "                                        )\n",
    "        \n",
    "        #create labels to give subwords the same class using word ids\n",
    "        word_ids = encoding.word_ids()\n",
    "        labels = self.subword_segment(word_ids, y_labels)\n",
    "        \n",
    "        encoding[\"labels\"] = labels\n",
    "        encoding[\"input_ids\"] = encoding[\"input_ids\"][0]\n",
    "        encoding[\"attention_mask\"] = encoding[\"attention_mask\"][0]\n",
    "        return encoding\n",
    "        \n",
    "    \n",
    "    # training method    \n",
    "    def train(self, epochs):\n",
    "        train_encodings = [self.encode_example(x,y) for x,y in zip(self.X_train, self.y_train)]\n",
    "        test_encodings = [self.encode_example(x,y) for x,y in zip(self.X_test, self.y_test)]\n",
    "        val_encodings = [self.encode_example(x,y) for x,y in zip(self.X_val, self.y_val)]\n",
    "\n",
    "        self.train_ds = Dataset.from_list(train_encodings)\n",
    "        self.test_ds = Dataset.from_list(test_encodings)\n",
    "        self.val_ds = Dataset.from_list(val_encodings)\n",
    "\n",
    "        # use tokens and attention mask as the inputs\n",
    "        input_ids = Input(shape=(self.max_length,), name=\"input_ids\", dtype=tf.int32)\n",
    "        attention_mask = Input(shape=(self.max_length,), name=\"attention_mask\", dtype=tf.int32)\n",
    "\n",
    "        # return the final 4 hidden layers from Bert, using uncased or cased to match with training data\n",
    "        sequence_outputs = BertLayer(model=\"bert-base-uncased\" if self.lower else \"bert-base-cased\") ((input_ids, attention_mask))\n",
    "\n",
    "        x = Dropout(0.3)(sequence_outputs)\n",
    "        x = TimeDistributed(Dense(256, activation='relu'))(x) # time distributed due to attention\n",
    "        x = Dropout(0.3)(x)\n",
    "        #x = TimeDistributed(Dense(64, activation='relu'))(x)\n",
    "        #x = Dropout(0.2)(x)\n",
    "        y = TimeDistributed(Dense(3, activation='softmax'))(x) #output\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=y)\n",
    "        self.model.summary()\n",
    "\n",
    "        # sample weights\n",
    "        sample_weights = np.ones_like(self.train_ds[\"labels\"]) \n",
    "        sample_weights[self.train_ds[\"labels\"]==0] = 0\n",
    "        sample_weights[self.train_ds[\"labels\"]==1] = 3\n",
    "        sample_weights[self.train_ds[\"labels\"]==2] = 1\n",
    "\n",
    "        # train the model\n",
    "        self.model.compile(\n",
    "            optimizer= Adam(3e-5), # learning rate\n",
    "            loss= SparseCategoricalCrossentropy(),\n",
    "            metrics= [SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "        self.train_tf = tf.data.Dataset.from_tensor_slices((\n",
    "            {\n",
    "                \"input_ids\": np.array(self.train_ds[\"input_ids\"], dtype=np.int32),\n",
    "                \"attention_mask\": np.array(self.train_ds[\"attention_mask\"], dtype=np.int32)\n",
    "            },\n",
    "            np.array(self.train_ds[\"labels\"]),\n",
    "            sample_weights.astype(np.float32))) #sample_weights added here\n",
    "\n",
    "        self.val_tf = tf.data.Dataset.from_tensor_slices((\n",
    "            {\n",
    "                \"input_ids\": np.array(self.val_ds[\"input_ids\"], dtype=np.int32),\n",
    "                \"attention_mask\": np.array(self.val_ds[\"attention_mask\"], dtype=np.int32)\n",
    "            },\n",
    "            np.array(self.val_ds[\"labels\"])))\n",
    "\n",
    "        callbacks = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=3, restore_best_weights=True) # implement early stopping to prevent overfitting\n",
    "        history = self.model.fit(self.train_tf.batch(self.batch), \n",
    "                            epochs=epochs, \n",
    "                            validation_data=self.val_tf.batch(self.batch),\n",
    "                            callbacks=callbacks\n",
    "                            )\n",
    "\n",
    "        # plot loss and accuracy over epochs\n",
    "        plt.plot(history.history['sparse_categorical_accuracy'], label = 'accuracy')\n",
    "        plt.plot(history.history['loss'], label = 'loss')\n",
    "        plt.plot(history.history['val_sparse_categorical_accuracy'], label = 'val_accuracy')\n",
    "        plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.plot()\n",
    "\n",
    "\n",
    "    # predict a raw text input\n",
    "    def predict_sequence(self,raw_text):\n",
    "        # tokenize and pad\n",
    "        encoding = self.tokenizer.encode_plus(raw_text.split(), \n",
    "                                        max_length=self.max_length, #est earlier\n",
    "                                        truncation=True, \n",
    "                                        padding=\"max_length\",\n",
    "                                        is_split_into_words=True,\n",
    "                                        return_token_type_ids=False, #don't need these\n",
    "                                        return_tensors='tf' #for tensorflow\n",
    "                                        )\n",
    "\n",
    "        # predict sequence\n",
    "        preds = self.model.predict({\"input_ids\": encoding[\"input_ids\"],\n",
    "                            \"attention_mask\": encoding[\"attention_mask\"]})\n",
    "        \n",
    "        pred_classes = np.argmax(preds, axis=-1)[0]\n",
    "        \n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0])\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "        pred_words = []\n",
    "        current_word_tokens = [] # keep track of current word vs word_id for the sake of subwords\n",
    "        current_word_id = None\n",
    "        for tok, label, wid in zip(tokens,pred_classes,word_ids):\n",
    "            if wid is None: # for special tags\n",
    "                continue\n",
    "\n",
    "            if wid != current_word_id: # new word\n",
    "                if current_word_id is not None and first_label == 2:\n",
    "                    pred_words.append(\"\".join(current_word_tokens))\n",
    "\n",
    "                current_word_id = wid\n",
    "                current_word_tokens = [tok.replace(\"##\", \"\")]\n",
    "                first_label = label\n",
    "                \n",
    "            else: # for subwords\n",
    "                current_word_tokens.append(tok.replace(\"##\",\"\"))\n",
    "            \n",
    "        if current_word_id is not None and first_label == 2:\n",
    "            pred_words.append(\"\".join(current_word_tokens))\n",
    "\n",
    "        # put sentence back together\n",
    "        pred_sentence = \" \".join(pred_words)\n",
    "        pred_sentence = re.sub(r' ([,.?!:;])',r'\\1',pred_sentence)\n",
    "        pred_sentence = re.sub(r\" ' \", r\"'\", pred_sentence)\n",
    "        \n",
    "        return pred_classes, pred_sentence, encoding\n",
    "\n",
    "    # print the confusion matrix \n",
    "    def print_cm(self, tens):\n",
    "        predictions = self.model.predict(tens.batch(self.batch))\n",
    "        y_pred = np.argmax(predictions, axis=-1).flatten()\n",
    "\n",
    "        # get gold values across batches and flatten\n",
    "        y_list = []\n",
    "        for batch in tens:\n",
    "            _, y_batch = batch   \n",
    "            y_list.append(y_batch.numpy()) \n",
    "        y = np.concatenate(y_list, axis=0) \n",
    "\n",
    "        # mask padded labels\n",
    "        mask = (y != 0)\n",
    "        y_pred = y_pred[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "        cm = confusion_matrix(y, y_pred, labels=[1,2])\n",
    "\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "\n",
    "        labels = ['Remove', 'Keep']\n",
    "\n",
    "        disp.ax_.set_title(\"Filter Model\")\n",
    "        disp.ax_.set_xticklabels(labels,rotation=90)\n",
    "        disp.ax_.set_yticklabels(labels)\n",
    "\n",
    "        # use sklearn metrics \n",
    "        print(f\"F1 score: {f1_score(y, y_pred)}\")\n",
    "        print(f\"Classification Report: \\n {classification_report(y, y_pred)}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # evaluate, test on example, and print confusion matrix\n",
    "    def show_eval(self):\n",
    "        test_tf = tf.data.Dataset.from_tensor_slices((\n",
    "            {\n",
    "                \"input_ids\": np.array(self.test_ds[\"input_ids\"], dtype=np.int32),\n",
    "                \"attention_mask\": np.array(self.test_ds[\"attention_mask\"], dtype=np.int32)\n",
    "            },\n",
    "            np.array(self.test_ds[\"labels\"])\n",
    "        ))\n",
    "\n",
    "        loss, accuracy = self.model.evaluate(test_tf.batch(self.batch))\n",
    "\n",
    "        print(f\"Test loss: {loss}\")\n",
    "        print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "        print(\"\\n-------------\\n\")\n",
    "\n",
    "        print(\"Testing on the sentence: i, uh, don't, i don't think that is true\")\n",
    "        print(\"Expected: i don't think that is true\")\n",
    "        print(f\"Result: {self.predict_sequence(\"i, uh, don't, i don't think that is true\")[1]}\")\n",
    "\n",
    "        self.print_cm(test_tf)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class putting together Emil's speech to text model and one of the above models, adding in llm calls as well\n",
    "class STOT():\n",
    "    def __init__(self,model,audio_predictions=[]):\n",
    "        self.model = model\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=APIKEY,
    "            )\n",
    "\n",
    "        self.word_error = load(\"wer\")\n",
    "        self.asr = pipeline(task=\"automatic-speech-recognition\",\n",
    "            tokenizer=WhisperTokenizer.from_pretrained(\"Model\"),\n",
    "            model=AutoModelForSpeechSeq2Seq.from_pretrained(\"Model\"),\n",
    "            feature_extractor=WhisperFeatureExtractor())\n",
    "\n",
    "        self.lower = self.model.lower\n",
    "        self.segmented = self.model.segmented\n",
    "        self.audio_predictions=audio_predictions\n",
    "        \n",
    "\n",
    "    # takes in audio data and returns the transcribed text\n",
    "    def transcribe(self, audio):\n",
    "        transcription = self.asr(audio)[\"text\"].lower() if self.lower else self.asr(audio)[\"text\"]\n",
    "        return self.model.pre_process_text(transcription) if self.model.segmented else transcription\n",
    "\n",
    "\n",
    "    # get more academic sentence\n",
    "    def call_api(self, text):\n",
    "        content = \"Give me a more academic way of saying the following sentence. Only return the relevant sentence and nothing else: \\n\" + text\n",
    "        # API Call\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"openai/gpt-oss-20b:free\",\n",
    "            messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content}\n",
    "                ]\n",
    "        )\n",
    "        \n",
    "        response = response.choices[0].message.content\n",
    "        print(f\"New Sentence: {response}\")\n",
    "\n",
    "\n",
    "    # eval a single audio input\n",
    "    def eval_single_input(self, num):\n",
    "        audio = list(test[\"audio\"])[num]\n",
    "        unfiltered_transcript = self.model.X_test[num]\n",
    "        filtered_transcript = self.model.y_test[num]\n",
    "\n",
    "        transcribed_audio = self.transcribe(audio)\n",
    "        print(f\"Audio Transcription: {transcribed_audio}\")\n",
    "        print(f\"True Transcription: {unfiltered_transcript}\")\n",
    "        print(f\"Word Error Rate: {self.word_error.compute(predictions=transcribed_audio, references=unfiltered_transcript)}\")\n",
    "        print(\"-------------------\")\n",
    "\n",
    "        classes_from_model, transcript_from_model, mod_enc = self.model.predict_sequence(transcribed_audio)\n",
    "        classes_from_data, transcript_from_data, data_enc = self.model.predict_sequence(unfiltered_transcript)\n",
    "\n",
    "        print(f\"Transcription After Filter: {transcript_from_model}\")\n",
    "        print(f\"Filtered Gold Text: {transcript_from_data}\")\n",
    "        print(f\"True Filter: {filtered_transcript}\")\n",
    "\n",
    "        # get true labels\n",
    "        if self.model.model_type == \"transformer\":\n",
    "            true_enc_mod = self.model.encode_example(transcribed_audio, filtered_transcript)\n",
    "            true_labels_mod = true_enc_mod[\"labels\"]\n",
    "            true_enc_data = self.model.encode_example(unfiltered_transcript, filtered_transcript)\n",
    "            true_labels_data = true_enc_data[\"labels\"]\n",
    "        else:\n",
    "            _, true_labels_mod = self.model.encode_example(transcribed_audio, filtered_transcript)\n",
    "            _, true_labels_data = self.model.encode_example(unfiltered_transcript, filtered_transcript)\n",
    "\n",
    "        # mask 0s\n",
    "        mask_model = (true_labels_mod != 0)\n",
    "        classes_from_model = classes_from_model[mask_model]\n",
    "        true_labels_mod = true_labels_mod[mask_model]\n",
    "\n",
    "        mask_data = (true_labels_data != 0)\n",
    "        classes_from_data = classes_from_data[mask_data]\n",
    "        true_labels_data = true_labels_data[mask_data]\n",
    "\n",
    "        # print accuracies\n",
    "        accuracy_mod = np.mean(classes_from_model == true_labels_mod)\n",
    "        print(f\"Total Model Accuracy: {accuracy_mod}, F1: {f1_score(classes_from_model,true_labels_mod)}\")\n",
    "\n",
    "        accuracy_dat = np.mean(classes_from_data == true_labels_data)\n",
    "        print(f\"Filter Model Accuracy: {accuracy_dat}, F1: {f1_score(classes_from_data,true_labels_data)}\")\n",
    "\n",
    "        # prompt the LLM to get a more academic response\n",
    "        #self.call_api(transcript_from_model)\n",
    "        \n",
    "\n",
    "    # eval entire dataset\n",
    "    def eval(self):\n",
    "        # get data\n",
    "        audio_data = list(test[\"audio\"])\n",
    "        intermediary_data = self.model.X_test\n",
    "        final_data = self.model.y_test\n",
    "\n",
    "        # transcribe audio if not already transcribed\n",
    "        if len(self.audio_predictions) == 0:\n",
    "            for i in range(len(audio_data)):\n",
    "                audio_pred = self.transcribe(audio_data[i])\n",
    "                self.audio_predictions.append(audio_pred)\n",
    "\n",
    "        # get word error rate\n",
    "        average_wer = self.word_error.compute(predictions=self.audio_predictions, references=intermediary_data)\n",
    "        print(f\"Average Word Error Rate of Stot Model: {average_wer}\")\n",
    "        \n",
    "        # measure accuracy of filter\n",
    "        if self.model.model_type == \"transformer\":\n",
    "            encoding = [self.model.encode_example(raw, clean) for raw, clean in zip(self.audio_predictions, final_data)]\n",
    "            enc_ds = Dataset.from_list(encoding)\n",
    "\n",
    "            enc_tf = tf.data.Dataset.from_tensor_slices((\n",
    "                {\n",
    "                    \"input_ids\": np.array(enc_ds[\"input_ids\"], dtype=np.int32),\n",
    "                    \"attention_mask\": np.array(enc_ds[\"attention_mask\"], dtype=np.int32)\n",
    "                },\n",
    "                np.array(enc_ds[\"labels\"]),\n",
    "            ))\n",
    "\n",
    "            loss, accuracy = self.model.model.evaluate(enc_tf.batch(self.model.batch))\n",
    "\n",
    "            self.model.print_cm(enc_tf)\n",
    "\n",
    "        else:\n",
    "            filter_input, filter_output = self.model.create_binary_dataset(self.audio_predictions, final_data)\n",
    "            loss, accuracy = self.model.model.evaluate(filter_input, filter_output)\n",
    "\n",
    "            self.model.print_cm(filter_input, filter_output)\n",
    "\n",
    "        print(f\"Final Accuracy: {accuracy}, Final Loss: {loss}\")\n",
    "\n",
    "        \n",
    "    # record audio and run through model\n",
    "    def input_audio(self,duration=5,sample_rate=16000):\n",
    "        print(\"Recording...\")\n",
    "        audio = sd.rec(int(duration*sample_rate), samplerate=sample_rate, channels=1)\n",
    "        sd.wait()\n",
    "        print(\"Stopped\")\n",
    "\n",
    "        audio = audio.squeeze()  # remove channel dimension\n",
    "\n",
    "        print(\"here\")\n",
    "\n",
    "        transcribed_audio = self.transcribe({\n",
    "            \"array\": audio,\n",
    "            \"sampling_rate\": sample_rate,\n",
    "        })\n",
    "\n",
    "        print(\"here\")\n",
    "\n",
    "        _, transcript_from_model, _ = self.model.predict_sequence(transcribed_audio)\n",
    "\n",
    "        print(transcript_from_model)\n",
    "\n",
    "        self.api_call(transcript_from_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = LSTMModel() \n",
    "mod1.run(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9865ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = TransformerModel(batch=8)\n",
    "mod2.run(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    }
   ],
   "source": [
    "# can load in and pass to multiple models\n",
    "\n",
    "word_error = load(\"wer\")\n",
    "asr = pipeline(task=\"automatic-speech-recognition\",\n",
    "    tokenizer=WhisperTokenizer.from_pretrained(\"Model\"),\n",
    "    model=AutoModelForSpeechSeq2Seq.from_pretrained(\"Model\"),\n",
    "    feature_extractor=WhisperFeatureExtractor())\n",
    "\n",
    "def transcribe(audio,lower=True,segmented=True):\n",
    "    transcription = asr(audio)[\"text\"].lower() if lower else asr(audio)[\"text\"]\n",
    "    return mod1.pre_process_text(transcription) if segmented else segmented\n",
    "\n",
    "audio_data = list(test[\"audio\"])\n",
    "intermediary_data = [x.lower() for x in list(test[\"transcript_a\"])]\n",
    "\n",
    "audio_predictions = []\n",
    "for i in range(len(audio_data)):\n",
    "    audio_pred = transcribe(audio_data[i])\n",
    "    audio_predictions.append(audio_pred)\n",
    "\n",
    "average_wer = word_error.compute(predictions=audio_predictions, references=intermediary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "stot = STOT(model=mod2, audio_predictions=audio_predictions)\n",
    "stot.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "stot.input_audio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schoool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
